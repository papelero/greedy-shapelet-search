{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan-temp/Desktop/General/Privat/greedy-shapelet-search/venv-hyp-mar/lib/python3.9/site-packages/mass_ts/_mass_ts.py:17: UserWarning: GPU support will not work. You must pip install mass-ts[gpu].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyts.datasets import load_gunpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mass_ts as mts\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import scipy.stats as sps\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from scipy.special import entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_gunpoint(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyShapeletSearch():\n",
    "    def __init__(self):\n",
    "        self.shapelets = []\n",
    "        self.exclusion_zone = {}\n",
    "        self.features = []\n",
    "        self.top_shapelets = []\n",
    "\n",
    "    @staticmethod\n",
    "    def rolling_window(a, window):\n",
    "        shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "        strides = a.strides + (a.strides[-1],)\n",
    "        return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "    \n",
    "    def get_candidate_mins(self, sample_data, shapelet_size = 10):\n",
    "        \"\"\"\n",
    "        Function that calculates the distance of all candidates of a given data set to all all other candidates.\n",
    "        CAREFUL:\n",
    "        - memory blows up quickly.\n",
    "        - contains the zeros (distance of candidates to itself)\n",
    "        \"\"\"\n",
    "        # Window the array\n",
    "        windowed_data = self.rolling_window(sample_data, shapelet_size)\n",
    "        # Standardize candidates\n",
    "        windowed_data = self.standardize_samples_candidates(windowed_data)\n",
    "        distances = []\n",
    "        for sample_candidates in windowed_data:\n",
    "            candidate_distances = np.array([((windowed_data - candidate)**2).sum(axis=-1).min(axis=-1) for candidate in sample_candidates])\n",
    "            distances.append(candidate_distances)\n",
    "        return np.stack(distances)\n",
    "    \n",
    "    def get_top_k_shapelets(self, X_train, y_train, scoring_function, n_shapelets=1, shapelet_min_size = 10, shapelet_max_size=20):\n",
    "\n",
    "        for _ in range(n_shapelets):\n",
    "            self.shapelets = []\n",
    "            self.main_event_loop(X_train, y_train, scoring_function, shapelet_min_size = shapelet_min_size, shapelet_max_size = shapelet_max_size)\n",
    "\n",
    "    def main_event_loop(self, X_train, y_train, scoring_function, shapelet_min_size = 30, shapelet_max_size = 31):\n",
    "        \"\"\"\n",
    "        The main event loop contains the series of steps required for the algorithm.\n",
    "        \"\"\"\n",
    "        for shapelet_size in range(shapelet_min_size, shapelet_max_size):\n",
    "            # Calculate all of the candidate minimums throughout the dataset - shape: n_samples, n_samples, n_candidate\n",
    "            profiles = self.get_candidate_mins(X_train, shapelet_size)\n",
    "            # Extract a shapelet for n_shapelets\n",
    "            self.evaluate_candidates(profiles, y_train, shapelet_size, scoring_function)\n",
    "        # Retrieve top shapelets\n",
    "        self.retrieve_top_shapelet(X_train)\n",
    "    \n",
    "    def evaluate_candidates(self, profiles, y_train, shapelet_size, scoring_function):\n",
    "        \"\"\"\n",
    "        Extracts a (greedy) optimal shapelet.\n",
    "        \"\"\"\n",
    "        # Iterate through all samples in profiles\n",
    "        for sample_idx in range(profiles.shape[0]):\n",
    "            # The minimum distances of the given samples candidates to all other samples - shape: n_samples, n_candidates\n",
    "            sample = profiles[sample_idx]\n",
    "            # Iterate through all candidate distances of the given sample\n",
    "            for candidate_idx in range(sample.shape[0]):\n",
    "                # The minimum distances of a candidate to all other samples\n",
    "                candidate = sample[candidate_idx,:]\n",
    "                # Add features if other shapelets have been extracted\n",
    "                features = self.get_features(candidate)\n",
    "                # Score candidate\n",
    "                score, margin = scoring_function(features, y_train)\n",
    "                self.shapelets.append((sample_idx, candidate_idx, score, margin, shapelet_size, candidate))\n",
    "                \n",
    "    def get_features(self,candidate):\n",
    "        \"\"\"\n",
    "        If shapelets have been extracted, returns the min distances of all already extracted shapelets + candidate as features\n",
    "        \"\"\"\n",
    "        if len(self.features) == 0:\n",
    "            return np.array(candidate).reshape((candidate.shape[0],1))\n",
    "        return np.array([candidate]+self.features).T\n",
    "\n",
    "    def retrieve_top_shapelet(self, X_train):\n",
    "        # Sort shapelets according to info gain descending\n",
    "        self.shapelets.sort(key=lambda x: (x[2],x[3]), reverse=True)\n",
    "\n",
    "        top_shapelet_found = False\n",
    "        for sample_idx, candidate_idx, score, margin, shapelet_size, candidate in self.shapelets:\n",
    "            # If the correct number of shapelets was found, break out of loop\n",
    "            if top_shapelet_found:\n",
    "                break\n",
    "            # Check if sample index of candidate in exclusion zone samples\n",
    "            if sample_idx not in self.exclusion_zone.keys():\n",
    "                self.exclusion_zone[sample_idx] = []\n",
    "            else:\n",
    "                # Otherwise check if candidate index is in exclusion zone of sample\n",
    "                if candidate_idx in self.exclusion_zone[sample_idx]:\n",
    "                    continue\n",
    "            # Extend exclusion zone\n",
    "            self.exclusion_zone[sample_idx].extend(list(range(candidate_idx - shapelet_size, candidate_idx+shapelet_size)))\n",
    "            # Add profile features to features\n",
    "            self.features.append(candidate)\n",
    "            # Add shapelet to top shapelets\n",
    "            self.top_shapelets.append((sample_idx, candidate_idx, score, margin, shapelet_size, X_train[sample_idx,candidate_idx:candidate_idx+shapelet_size]))\n",
    "            # Signal shapelet found\n",
    "            top_shapelet_found = True\n",
    "\n",
    "\n",
    "    def calculate_infogain(self, candidate, y_train, target_class = 1):\n",
    "        \"\"\"\n",
    "        Given a 1-d array, calculates the infogain according the labels y_train.\n",
    "        \"\"\"\n",
    "        # Initialize decision tree classifier\n",
    "        clf = DecisionTreeClassifier(random_state=0, criterion='entropy', max_depth=1)\n",
    "        # Fit decision tree\n",
    "        clf.fit(candidate, y_train)\n",
    "        self.clf=clf\n",
    "        # Get entropy before best split\n",
    "        entropy_before = clf.tree_.impurity[0]\n",
    "        # Get entropy after best split\n",
    "        entropy_after = clf.tree_.value.sum(-1)[1]/clf.tree_.value.sum(-1)[0] * clf.tree_.impurity[1] + \\\n",
    "            clf.tree_.value.sum(-1)[2]/clf.tree_.value.sum(-1)[0] * clf.tree_.impurity[2]\n",
    "\n",
    "        # Calculate margin\n",
    "        if len(set(y_train)) != 2:\n",
    "            print(f\"There is something wrong with the number of labels! The number o labels is {len(set(y_train))}.\")\n",
    "            raise ValueError\n",
    "        label_avgs = [candidate[y_train==label].mean() for label in set(y_train)]\n",
    "        margin = abs(label_avgs[0]-label_avgs[1])\n",
    "        # Return information gain\n",
    "        return entropy_before - entropy_after, margin\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize_samples_candidates(samples, axis=2):\n",
    "        \"\"\"\n",
    "        Standardized each shapelet candidate (after windowing).\n",
    "        \"\"\"\n",
    "        return (samples-np.expand_dims(samples.mean(axis=axis),axis))/np.expand_dims(samples.std(axis=axis),axis)\n",
    "\n",
    "    \n",
    "\n",
    "GSS = GreedyShapeletSearch()\n",
    "GSS.get_top_k_shapelets(X_train, y_train, scoring_function=fit_svm, n_shapelets=2, shapelet_min_size=30, shapelet_max_size=40)\n",
    "# ST.main_event_loop(X_train, y_train, n_shapelets=5, shapelet_size=38)\n",
    "# GSS.main_event_loop(X_train, y_train, scoring_function=GSS.calculate_infogain, shapelet_min_size=30, shapelet_max_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSS.top_shapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# result_mapping = {}\n",
    "# time_mapping = {}\n",
    "# for i in range(5):\n",
    "#     start = time.time()\n",
    "#     GSS = GreedyShapeletSearch()\n",
    "#     GSS.get_top_k_shapelets(X_train, y_train, scoring_function=fit_svm, n_shapelets=40, shapelet_min_size=30, shapelet_max_size=40)\n",
    "#     result_mapping[i] = GSS.top_shapelets\n",
    "#     time_mapping[i] = time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('time_mapping.json', 'w') as f:\n",
    "#     json.dump(time_mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSS.top_shapelets\n",
    "# GSS.\n",
    "\n",
    "# GSS.shapelets\n",
    "# GSS.top_shapelets = []\n",
    "# GSS.top_shapelets.append((8,105,0,0,0,39))\n",
    "\n",
    "\n",
    "# profiles = GSS.get_candidate_mins(X_train, shapelet_size=39)\n",
    "# candidate = profiles[8,105,:]\n",
    "# fit_svm(candidate,y_train)\n",
    "\n",
    "\n",
    "# plt.plot(candidate[y_train==1],color = 'red')\n",
    "# plt.plot(candidate[y_train==2],color = 'blue')\n",
    "\n",
    "# print(max(candidate[y_train==2]))\n",
    "# print(min(candidate[y_train==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9387755102040817"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "def features_transform(X):\n",
    "    \"\"\"\n",
    "    Transformation pipeline for a new dataset X\n",
    "    \"\"\"\n",
    "    features =  []\n",
    "    for _, _, _, _, shapelet_size, shapelet in GSS.top_shapelets[:3]:\n",
    "        # Normalize shapelet\n",
    "        shapelet_norm = GSS.standardize_samples_candidates(shapelet, axis=0)\n",
    "        # Window the data\n",
    "        windowed_test = GSS.rolling_window(X, window=shapelet_size)\n",
    "        # Normalize the windowed data\n",
    "        windowed_test_norm = GSS.standardize_samples_candidates(windowed_test)\n",
    "        # Calculate features for shapelet and X\n",
    "        shapelet_features = ((windowed_test_norm-shapelet_norm)**2).sum(axis=-1).min(axis=-1)\n",
    "        features.append(shapelet_features)\n",
    "    return np.array(features).T\n",
    "\n",
    "# features = features_transform(X_train, X_train)\n",
    "# print(features.shape)\n",
    "# print(features)\n",
    "\n",
    "\n",
    "def fit_classifier(X_train, y_train, X_test, y_test, classifier, scoring_function):\n",
    "    \"\"\"\n",
    "    Tests the performance of a classifier that is first trained  and then tested according to a specified scoring function.\n",
    "    \"\"\"\n",
    "    # Apply the feature pipeline to the training set and testing set to get the min distances of each shapelet\n",
    "    features_train = features_transform(X_train)\n",
    "    features_test = features_transform(X_test)\n",
    "    # Normalizing min distances\n",
    "    features_train_norm, features_test_norm = feature_normalization(features_train, features_test)\n",
    "    classifier.fit(features_train_norm, y_train)\n",
    "    y_pred = classifier.predict(features_test_norm)\n",
    "    return scoring_function(y_test, y_pred)\n",
    "\n",
    "def feature_normalization(features_train, features_test):\n",
    "    features_train_norm = (features_train-features_train.mean(axis=0))/features_train.std(axis=0)\n",
    "    features_test_norm = (features_test-features_train.mean(axis=0))/features_train.std(axis=0)\n",
    "    return features_train_norm, features_test_norm\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# clf = SVC(kernel='linear')\n",
    "clf = make_pipeline(StandardScaler(), SVC(kernel='linear',class_weight='balanced'))\n",
    "\n",
    "\n",
    "\n",
    "fit_classifier(X_train, y_train, X_test, y_test, clf, f1_score)\n",
    "\n",
    "# def extract_shapelets(X_train, sample_idx, candidate_idx, shapelet_size):\n",
    "#     \"\"\"\n",
    "#     Given a shapelet_size and a given shapelet, computes the profiles \n",
    "#     \"\"\"\n",
    "#     # Edtract shapelet from data \n",
    "#     shapelet = X[sample_idx, candidate_idx:candidate_idx+shapelet_size]\n",
    "#     shapelets_norm = GSS.standardize_samples_candidates(shapelet.reshape(-1,1),axis=0).T\n",
    "#     windowed_test = GSS.rolling_window(X, window=shapelet_size)\n",
    "#     windowed_test_norm = GSS.standardize_samples_candidates(windowed_test)\n",
    "#     feature = (windowed_test_norm-shapelets_norm).sum(axis=-1).min(axis=-1)\n",
    "#     return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def fit_svm(X,Y, target_class=1):\n",
    "    \"\"\"\n",
    "    Fitting a SVM and returning the f1 score and the calculated margin.\n",
    "    \"\"\"\n",
    "    # Initialize the classifier\n",
    "    clf = SVC(kernel='linear', class_weight='balanced')\n",
    "    # Adjust the dimensions of X if necessary\n",
    "    if len(X.shape) == 1:\n",
    "       X = X.reshape(-1, 1) \n",
    "    # Normalize the input data\n",
    "    X_norm = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "    # Fit SVM\n",
    "    clf.fit(X_norm, Y)\n",
    "    # Calculate margin\n",
    "    margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n",
    "    # Predict\n",
    "    Y_pred = clf.predict(X_norm)\n",
    "    # Calculate info gain\n",
    "    entropy_before = calculate_entropy(Y)\n",
    "    # Start with a 'pure' entropy \n",
    "    entropy_after = 0\n",
    "    # Iterating through classes\n",
    "    for label in set(Y):\n",
    "        # Retrieve the true labels of all instances classified as 'label'\n",
    "        partial_data = Y[Y_pred == label]\n",
    "        # Add the weighted entropy to the entropy after\n",
    "        entropy_after += len(partial_data)/len(Y_pred) * calculate_entropy(partial_data)\n",
    "\n",
    "    return entropy_before-entropy_after, margin\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the entropy of a data set.\n",
    "    \"\"\"\n",
    "    pd_series = pd.Series(data)\n",
    "    counts = pd_series.value_counts()\n",
    "    entropy = sps.entropy(counts, base=2)\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(calculate_entropy(y_train))\n",
    "print(np.sum(entr(y_train))/np.log(2))\n",
    "print( -np.sum(y_train*np.log2(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(kernel='linear',class_weight='balanced'))\n",
    "clf['svc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0, criterion='entropy', max_depth=1)\n",
    "# Fit decision tree\n",
    "clf.fit(X_train[:,0].reshape(50,1), y_train)\n",
    "# Get entropy before best split\n",
    "entropy_before = clf.tree_.impurity[0]\n",
    "\n",
    "print(entropy_before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}